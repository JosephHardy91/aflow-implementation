{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffac8e67",
   "metadata": {},
   "source": [
    "# ${Aflow}$ Implementation experiments\n",
    "\n",
    "## Source\n",
    "* https://arxiv.org/pdf/2410.10762\n",
    "\n",
    "## Notes\n",
    "* Use gpt-4o-mini as executor/workflow constructor (executor/optimizer)\n",
    "* backprop through modifications (?) that are descriptions for the executor to implement onto the parent\n",
    "     * therefore need a way to modify the parent via text\n",
    "\n",
    "## Algorithm\n",
    "* Split data into 20% val/80% training\n",
    "* $async$ Get scores for initial workflow on val data\n",
    "* Pare down val data to examples with high score variance (above threshold)\n",
    "* set $experiences = []$\n",
    "* set $allresults = []$\n",
    "* set $topkW = []$\n",
    "* set $W^* = None$\n",
    "* set $topkScores = []$\n",
    "* set $bestScore = 0$\n",
    "\n",
    "* for each training step,\n",
    "    * set parent as initial workflow for first step or select parent from last training step results using subroutine ${SelectParent}$\n",
    "    * get the optimizer context (parent workflow + last step experiences)\n",
    "    * use the optimizer with the context and set of available operators to improve the parent workflow and generate $W_{round}$ and the ${modification}$ it's based on\n",
    "    * set $evalresults=[]$\n",
    "    * $async$ for each of 5 rounds\n",
    "        * execute $W_{round}$ on the val data and get ${score}/{cost}$ from evaluator ${E} / {G}$\n",
    "        * append the ${score},{cost}$ to $evalresults$\n",
    "    * append $evalresults$ to $allresults$\n",
    "    * calculate the average $score$ for the 5 rounds from $evalresults$\n",
    "    * create a new experience with the parent workflow, the modification to produce $W_{round}$, and the average $score$\n",
    "    * append the experience to $experiences$ \n",
    "    * if the average $score$ is higher than the $bestScore$, update $W^* = W_{round}$ and $bestScore = $ average $score$\n",
    "    * add $W_{round}$ to $topkW$ and average $score$ to $topkScores$ if $topkW$ has fewer than $k$ elements or average $score$ is greater than the lowest value in $topkScores$\n",
    "    * if $topkScores$ hasn't changed in specified (n) number of rounds, stop early and return $W^*$\n",
    "* return $W^*$\n",
    "    \n",
    "### Algorithm Unknowns\n",
    "* how to get evaluator G/E?\n",
    "* how do initial scores get computed? are there multiple rounds of scoring per example in order to generate variance samples?\n",
    "* how does the optimizer generate new workflows from previous samples (experiences)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28c83b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import e\n",
    "from random import choices,seed,random\n",
    "from uuid import uuid5 as uuid\n",
    "from uuid import NAMESPACE_DNS\n",
    "from copy import deepcopy\n",
    "from pydantic import BaseModel,conint\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965f9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# DONE: Implement create_experience\n",
    "class Experience:\n",
    "    def __init__(self,workflow,round_added,score,parent_score=None,modification=None):\n",
    "        self.workflow = workflow\n",
    "        \n",
    "        self.parent_workflow = self.workflow.parent\n",
    "        if self.parent_workflow:\n",
    "            self.parent_workflow_id = self.parent_workflow.id\n",
    "        else:\n",
    "            self.parent_workflow_id = None\n",
    "        self.parent_score = parent_score\n",
    "        \n",
    "        if modification:\n",
    "            self.round_modification_added = round_added\n",
    "            self.round_added = self.parent_workflow.round_added\n",
    "        else:\n",
    "            self.round_modification_added = None\n",
    "            self.round_added = round_added\n",
    "        \n",
    "        self.score = score\n",
    "        self.modification = modification\n",
    "        \n",
    "        if self.parent_score and (isinstance(self.parent_score,float) or isinstance(self.parent_score,int)):\n",
    "            self.success = self.score > self.parent_score\n",
    "        else:\n",
    "            self.success = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Experience Info:\n",
    "        Metadata:\n",
    "        - Workflow with modification ID: {self.workflow.id}\n",
    "        - Workflow with modification score: {self.score}\n",
    "        - Workflow without modification ID: {self.parent_workflow_id}\n",
    "        - Workflow without modification score: {self.parent_score}\n",
    "        \n",
    "        \n",
    "        Modification that lead to this experience:\n",
    "        - Did the modification lead to improvement over workflow without modifications: {self.success}\n",
    "        - Round Workflow was modified: {self.round_added}\n",
    "        - Modification details: {self.modification}\n",
    "        \"\"\"\n",
    "\n",
    "# DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "class Experiences:\n",
    "    def __init__(self):\n",
    "        self.experiences = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.experiences[key]\n",
    "        \n",
    "    def create_experience(self,workflow,round_,score,modification=None):\n",
    "        if round_ not in self.experiences:\n",
    "            self.experiences[round_] = {}\n",
    "        parent_score = None\n",
    "        if modification:\n",
    "            parent_score = self.experiences[workflow.parent.round_added][workflow.parent.id].score\n",
    "        self.experiences[round_][workflow.id] = Experience(workflow,round_,score,parent_score,modification)\n",
    "        return self.experiences[round_][workflow.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941f12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WorkflowNode:\n",
    "    def __init__(self,description,prompt,modification,next_node=None):\n",
    "        self.description = description\n",
    "        self.prompt = prompt\n",
    "        self.modification = modification\n",
    "        if next_node is None:\n",
    "            self.next = FinalNode()\n",
    "        else:\n",
    "            self.next = next_node()\n",
    "        \n",
    "    def __call__(self,**kwargs):\n",
    "        #does something with prev_step and returns it\n",
    "        llm_client = kwargs['llm_client']\n",
    "        data = kwargs['data']\n",
    "        \n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "    \n",
    "class FinalNode:\n",
    "    def __init__(self):\n",
    "        self.next = None\n",
    "        self.description = 'This node is a final sentinel and simply passes what it\\'s given through.'\n",
    "        \n",
    "    async def __call__(self,**kwargs):\n",
    "        #print(kwargs)\n",
    "        return kwargs\n",
    "    \n",
    "    def copy(self):\n",
    "        return self\n",
    "    \n",
    "class Workflow:\n",
    "    #contains all of the information needed to understand a single workflow\n",
    "    def __init__(self,round_added = 0,parent = None):\n",
    "        self.next = FinalNode()\n",
    "        self.round_added = round_added\n",
    "        self.copies = 0\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        self.id = uuid(NAMESPACE_DNS,str(self.round_added)+chr(97+self.copies))\n",
    "        self.parent = parent\n",
    "        \n",
    "    async def start(self,first_inputs):\n",
    "        return await self.next(**first_inputs)\n",
    "        \n",
    "    def add_node(self,node_class:WorkflowNode,description,prompt,modification):\n",
    "        if not isinstance(self.next,FinalNode):\n",
    "            current_node = self.next\n",
    "            while current_node.next:\n",
    "                current_node = current_node.next\n",
    "            #print(type(current_node))\n",
    "        else:\n",
    "            current_node = self\n",
    "        new_node = node_class(description,prompt,modification)\n",
    "        current_node.next = new_node\n",
    "        return self\n",
    "        \n",
    "    def copy(self):\n",
    "        # Create a shallow copy of the current instance\n",
    "        new_workflow = Workflow(self.round_added,self)\n",
    "        new_workflow.next = self.next.copy()\n",
    "        new_workflow.copies = self.copies + 1\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        new_workflow.id = uuid(NAMESPACE_DNS, str(new_workflow.round_added) + chr(97 + new_workflow.copies))\n",
    "        return new_workflow \n",
    "    \n",
    "    def extract_chain_info(self):\n",
    "        chain = []\n",
    "        chain_info = []\n",
    "        current_node = self.next\n",
    "        while current_node.next:\n",
    "            chain.append(type(current_node))\n",
    "            chain_info.append(current_node.description)\n",
    "            current_node = current_node.next\n",
    "            \n",
    "        return ' -->> '.join(map(str,chain)),'\\n'.join([':'.join((node,node_info)) for node,node_info in zip(map(str,chain),chain_info)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f910299-a82e-4e63-8d00-a5c335ae64b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#available operators\n",
    "# class GeneratePrompts(WorkflowNode):\n",
    "#     def __init__():\n",
    "#         super().__init__()\n",
    "#     def __call__(self,**kwargs):\n",
    "#         #does something with prev_step and returns it\n",
    "#         llm_client = kwargs['llm_client']\n",
    "#         data = kwargs['data']\n",
    "#         prompt\n",
    "#         results = []\n",
    "#         for instance in data:\n",
    "#             llm_client.chat.completions.create(\n",
    "#                 messages=[\n",
    "#                     messages+[\n",
    "#                         {\"role\":\"user\"},\n",
    "#                         {\"content\":instance}\n",
    "#                     ]\n",
    "#                 ],\n",
    "#                 model='gpt-4o-mini'\n",
    "#             ).messages[0].content\n",
    "#         self.next(messages=results,data=data,\n",
    "#                   llm_client=llm_client)        \n",
    "#     def copy(self):\n",
    "#         return deepcopy(self)\n",
    "    \n",
    "class GenerateText(WorkflowNode):\n",
    "    def __init__(self,description,prompt,modification):\n",
    "        super().__init__(description,prompt,modification)\n",
    "        \n",
    "    async def __call__(self,**kwargs):\n",
    "        #does something with prev_step and returns it\n",
    "        llm_client = kwargs.get('llm_client',None)\n",
    "        data = kwargs.get('data',[])\n",
    "        results = []\n",
    "        #print('in node:',llm_client,';',data)\n",
    "        for instance in data:\n",
    "            result = (await llm_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":self.prompt},\n",
    "                    {\"role\":\"user\",\"content\":instance}\n",
    "                ],\n",
    "                model='gpt-4o-mini'\n",
    "            )).choices[0].message.content\n",
    "            results.append(result)\n",
    "        return await self.next(data=results,llm_client=llm_client)\n",
    "        \n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8e6bd-57d9-4a02-ba7c-5079f0d60d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d054de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement split_data\n",
    "def split_data(dataset,val_ratio=0.2,random_seed=1337):\n",
    "    seed(random_seed)\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    for datum in dataset:\n",
    "        if random() < val_ratio:\n",
    "            val_set.append(datum)\n",
    "        else:\n",
    "            test_set.append(datum)\n",
    "            \n",
    "    return val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec8c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement determine_variance_threshold\n",
    "# DONE: Implement select_high_variance_instances\n",
    "# TODO: Handle asynchronous running (e.g. each output score set val data i may not correspond with input val data i)\n",
    "def select_high_variance_instances(val_data,score_sets,marginal_variance_tolerance_perc = 0.05):\n",
    "    variances = []\n",
    "    n_data_points = len(score_sets[0])\n",
    "    for i in range(n_data_points):\n",
    "        scores_i = [score_set[i] for score_set in score_sets]\n",
    "        num_scores_i = len(scores_i)\n",
    "        avg_score_i = sum(scores_i)/num_scores_i\n",
    "        sample_variance_i = (1/num_scores_i)*sum((score_i-avg_score_i)**2 for score_i in scores_i)\n",
    "        variances.append(sample_variance_i)\n",
    "        \n",
    "    sorted_variances = sorted(list(set(variances)),reverse=True)\n",
    "    #use elbow method diminishing gains based on marginal_variance_tolerance_perc\n",
    "    last_variance = None\n",
    "    for variance in sorted_variances:\n",
    "        if last_variance is None:\n",
    "            last_variance = variance\n",
    "        else:\n",
    "            perc_change = (variance/last_variance)-1\n",
    "            if perc_change < marginal_variance_tolerance_perc:\n",
    "                return last_variance\n",
    "            last_variance = variance\n",
    "            \n",
    "    return [datum for i,datum in enumerate(val_data) if variances[i]>last_variance],last_variance\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3b635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement select_parent\n",
    "def select_parent(topk_scores,topk_W):\n",
    "    probabilities = calculate_mixed_probabilities(topk_scores) #DONE: Implement calculate_mixed_probabilities\n",
    "    return sample_from_categorical(probabilities,topk_W) #DONE: Implement sample_from_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c0bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement calculate_mixed_probabilities\n",
    "def calculate_mixed_probabilities(scores,lambda_=0.4,alpha=0.5): #TODO: Determine appropriate values of lambda and alpha from the paper\n",
    "    n = len(scores)\n",
    "    max_score = max(scores)\n",
    "    w = list(e**(alpha*(s_i-max_score)) for s_i in scores)\n",
    "    P_score = (w_i/sum(w) for w_i in w)\n",
    "    P_uniform = (1/n for _ in range(n))\n",
    "    P_mixed = (lambda_ * P_uniform_i + (1-lambda_) * P_score_i for P_score_i,P_uniform_i in zip(P_score,P_uniform))\n",
    "    return P_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f425c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement sample_from_categorical\n",
    "def sample_from_categorical(probabilities,workflows):\n",
    "    return choices(population=workflows,weights=probabilities,k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d24de-28ac-4e17-9c8f-b28674131ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list(calculate_mixed_probabilities([0.33,0.233]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d4239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement load_context\n",
    "def load_context(parent_workflow,experiences):\n",
    "    #get the experiences related to parent_workflow\n",
    "    #parent_workflow may or may not have modifications\n",
    "    experience_info = []\n",
    "    for round_ in experiences:\n",
    "        if parent_workflow.id in experiences[round_]:\n",
    "            experience_info.append(str(experiences[round_][parent_workflow.id]))\n",
    "    workflow_experiences = '\\n\\n'.join(experience_info)\n",
    "    \n",
    "    #get the node chain visualization and node descriptions parent_workflow implements\n",
    "    chain_viz,chain_descriptions = parent_workflow.extract_chain_info()\n",
    "    \n",
    "    context_string = f\"\"\"\n",
    "        Context for workflow {parent_workflow.id}:\n",
    "        \n",
    "        Workflow node chain visualization:\n",
    "        {chain_viz}\n",
    "        \n",
    "        Descriptions of each node:\n",
    "        {chain_descriptions}\n",
    "        \n",
    "        Trials (experiences) involving this workflow:\n",
    "        {workflow_experiences}\n",
    "    \"\"\"\n",
    "    return context_string\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c480c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement optimize\n",
    "class NodeAdd(BaseModel):\n",
    "    node:str\n",
    "    modification:str\n",
    "    description:str\n",
    "    prompt:str\n",
    "    \n",
    "async def optimize(parent_workflow,context_string,dict_of_valid_operations,llm_client):\n",
    "    C,O = context_string,'\\n\\n'.join(map(str,dict_of_valid_operations.keys())) #TODO: Implement Node __str__ for this purpose\n",
    "    \n",
    "    optimize_system_prompt = \"\"\"\n",
    "    You are being given a chance to improve the workflow used to answer user queries.\n",
    "    You will be given information about the workflows as well as information about, and results of, previous attempts (if applicable) to improve the workflow.\n",
    "    You will also be given information about valid operations that can be added to the workflow in order to increase its ability to successfully answer the user query.\n",
    "    \n",
    "    You will be expected to reason step-by-step in order to come to a conclusion about what node (valid operation) to add to the workflow and the prompt that should direct it.\n",
    "    \n",
    "    At the end of your reasoning steps, you should output a single json object in the following format:\n",
    "    {\"node\":\"<NAME OF NODE>\",\"modification\":\"Add a new <NAME OF NODE> to <AND SO ON>\",\"description\":\"<DESCRIPTION>\",\"prompt:\"<PROMPT>\"}\n",
    "    where \"<NAME OF NODE>\" is one of given node names. If you fail to choose an existing node name, you will fail your task.\n",
    "    And where <AND SO ON> is where the rest of your explanation should go. If you fail to rationally, simply and clearly describe \n",
    "    your reasoning here, you will fail your task and future optimizations will go poorly.\n",
    "    Third, <DESCRIPTION> is your description of the node's purpose and how it is intended to go about its purpose. This is critical for communicating to yourself in the future about why you created a certain node, so be detailed.\n",
    "    And, finally, <PROMPT>, which is where you exercise your best ability to optimize and improve the results of the next node. If you fail to rationally, simply, and clearly describe\n",
    "    how the next node should interact with its data (which follows from the last node in the current workflow), the optimization task will fail utterly.\n",
    "    \n",
    "    You are OPTIMIZER.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimize_user_prompt = f\"\"\"\n",
    "    Here is your context:\n",
    "    Workflow information:\n",
    "    {C}\n",
    "    \n",
    "    Valid operations/nodes:\n",
    "    {O}\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(optimize_user_prompt)\n",
    "    \n",
    "    client_response = await llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":optimize_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":optimize_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o-mini',\n",
    "        response_format=NodeAdd,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    \n",
    "    nodeadd_obj = client_response.choices[0].message.parsed\n",
    "    \n",
    "    print(nodeadd_obj.dict())\n",
    "    \n",
    "    W_round = parent_workflow.copy().add_node(dict_of_valid_operations.get(nodeadd_obj.node),nodeadd_obj.description,nodeadd_obj.prompt,nodeadd_obj.modification)\n",
    "    return W_round,nodeadd_obj.modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84e5a8-fc1d-4fa5-a0b8-136234bf7321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# na = NodeAdd(node = '1',description = '2',prompt = '3',modification = '4')\n",
    "# # na.node = '1'\n",
    "# # na.description = '2'\n",
    "# # na.prompt = '3'\n",
    "# # na.modification = '4'\n",
    "# na.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8da5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement evaluator \n",
    "#will use gpt-4o to assess results (small dataset for now)\n",
    "class Evaluation(BaseModel):\n",
    "    grade:int#conint(ge=0,le=10)\n",
    "        \n",
    "async def evaluate(question,correct_answer,given_answer):\n",
    "    evaluate_system_prompt = \"\"\"\n",
    "    You are being given a chance to evaluate the result of a workflow used to answer user queries.\n",
    "    You will be given the user query, the correct answer, the the answer provided by the workflow.\n",
    "    \n",
    "    At the end of your succinct explanation, you should output only a single json object in the following format:\n",
    "    {\"grade\":<GRADE>}\n",
    "    where <GRADE> is an integer between 0 and 10, where 0 is completely irrelevant and/or completely unintelligible, and 10 is perfect in both content and understandability. In between is a continuum of integer grades describing differing levels of imperfection. \n",
    "    \n",
    "    You are EVALUATOR.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_user_prompt = f\"\"\"\n",
    "    Here is the user query:\n",
    "    {question}\n",
    "    \n",
    "    Here is the correct answer:\n",
    "    {correct_answer}\n",
    "    \n",
    "    Here is the answer generated by the workflow:\n",
    "    {given_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    client_response = await llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":evaluate_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":evaluate_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o',\n",
    "        response_format=Evaluation,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    \n",
    "    evaluation_obj = client_response.choices[0].message.parsed\n",
    "    return evaluation_obj.grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b41c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement execute_workflow_over_data\n",
    "# TODO: Make this asynchronous\n",
    "# TODO: Implement categorical loss\n",
    "async def get_workflow_data_score(workflow,evaluator,datum,llm_client):\n",
    "    q,a = datum\n",
    "    a_hat = (await workflow.start({'data':[q],'llm_client':llm_client}))['data']\n",
    "    score = await evaluator(q,a,a_hat)\n",
    "    return score\n",
    "\n",
    "async def execute_workflow_over_data(workflow,evaluator,data,llm_client):\n",
    "    #scores = []\n",
    "    #costs = []\n",
    "    data_tasks = []\n",
    "    for datum in data:\n",
    "        data_tasks.append(get_workflow_data_score(workflow,evaluator,datum,llm_client))\n",
    "         #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "    scores = await asyncio.gather(*data_tasks)\n",
    "        #costs.append(cost)\n",
    "    return scores#,sum(costs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8319b29-7c67-4149-a622-b4663c66ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_round_scores(W_round,G,Dv,llm_client):\n",
    "    scores = await execute_workflow_over_data(W_round,G,Dv,llm_client) #DONE: Implement execute_workflow_over_data\n",
    "    round_score = sum(scores)/len(scores) #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "    return round_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebce8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def get_optimal_workflow(initial_workflow,evaluator,dataset,operators,llm_client,number_of_rounds=20,k=3,early_stopping_rounds=5,scoring_rounds=5):\n",
    "    #DONE: Implement evaluator\n",
    "    #Initialize variables\n",
    "    W_0, G, D, N, O, k, n, I = initial_workflow,evaluator,dataset,number_of_rounds,operators,k, \\\n",
    "                                early_stopping_rounds,scoring_rounds\n",
    "    results = []\n",
    "    experiences = Experiences()\n",
    "    topk_W = []\n",
    "    best_W = None\n",
    "    topk_scores = []\n",
    "    best_score = 0\n",
    "    topk_W_unchanged = 0\n",
    "    \n",
    "    #Split dataset\n",
    "    if len(dataset)<50:\n",
    "        Dv,Dt = dataset,dataset\n",
    "    else:\n",
    "        Dv,Dt = split_data(dataset) #DONE: Implement split_data\n",
    "    \n",
    "    #Gather initial scores to determine final validation dataset\n",
    "    score_set_tasks = [execute_workflow_over_data(W_0,G,Dv,llm_client) for _ in range(k)] #DONE: Implement execute_workflow_over_data\n",
    "    score_sets = asyncio.gather(*score_set_tasks)\n",
    "    if len(Dv)>=50:\n",
    "        Dv,variance_threshold = select_high_variance_instances(Dv,score_sets)   #DONE: Implement select_high_variance_instances\n",
    "                                                                        #DONE: Implement determine_variance_threshold\n",
    "    \n",
    "        print(f'{len(Dv)} validation samples selected based on variance threshold {variance_threshold:.2f}')\n",
    "    else:\n",
    "        print(f'All {len(Dv)} validation samples selected for training')\n",
    "    #MAIN LOOP----------------------------------------\n",
    "    #Iterate to improve bestScore\n",
    "    for round_ in tqdm(range(N)):\n",
    "        if round_ == 0:\n",
    "            parent = W_0\n",
    "        else:\n",
    "            print(topk_W,topk_scores)\n",
    "            if len(topk_W) == 1:\n",
    "                parent = topk_W[0]\n",
    "            else:\n",
    "                parent = select_parent(topk_scores,topk_W) #DONE: Implement select_parent\n",
    "        \n",
    "        #Load context for parent and perform optimization forward pass\n",
    "        if round_>0:\n",
    "            context = load_context(parent,experiences.experiences) #DONE: Implement load_context\n",
    "            W_round, modification = await optimize(parent,context,O,llm_client) #DONE: Implement optimize\n",
    "        else:\n",
    "            W_round,modification = parent,''\n",
    "        #Generate validation scores for modified workflow to demonstrate relative performance\n",
    "        round_score_tasks=[]\n",
    "        for i in range(I):\n",
    "            round_score_tasks.append(get_round_scores(W_round,G,Dv,llm_client))\n",
    "        round_scores = await asyncio.gather(*round_score_tasks)\n",
    "        for round_score in round_scores:\n",
    "            results.append((round_,round_score))\n",
    "        \n",
    "        #Capture a new experience for use in future optimization passes by using the modified workflow validation scores as gain\n",
    "        round_scores = [r[1] for r in results if r[0]==round_]\n",
    "        average_score = sum(round_scores)/len(round_scores)\n",
    "        #print(average_score)\n",
    "        experience = experiences.create_experience(W_round,round_,average_score,modification) #DONE: Implement create_experience\n",
    "#         experiences.append(experience) #DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "        \n",
    "        #Save the previous topk_W for the later early-stopping check\n",
    "        prev_topk_W = topk_W\n",
    "        \n",
    "        #Update best W, if applicable this round\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_W = W_round\n",
    "            \n",
    "        topk_W.append(W_round)\n",
    "        topk_scores.append(average_score)\n",
    "        \n",
    "        #Try to include this round's W_round into topk_W if the W_round's score is within the top k scores\n",
    "        combined = sorted(zip(topk_scores,topk_W),reverse=True)\n",
    "        topk_scores[:],topk_W[:] = zip(*combined)\n",
    "        \n",
    "        #Boot any workflows outside of the top k scores\n",
    "        if len(topk_scores)>k:\n",
    "            topk_scores = list(topk_scores[:-1])\n",
    "            topk_W = list(topk_W[:-1])\n",
    "            \n",
    "        #Check if topk_W is the same as the prev topk_W and increment the early stopping counter if so. If not, set counter to 0.\n",
    "        if set([W.id for W in topk_W])==set([prevW.id for prevW in prev_topk_W]):\n",
    "            topk_W_unchanged+=1 #DONE: make sure workflows are being compared based on their IDs \n",
    "                                                    #(since there may not be consistent parity between naive objects)\n",
    "        else:\n",
    "            topk_W_unchanged = 0\n",
    "        \n",
    "        #Stop the workflow improvement early if no new workflows have joined the top k workflows in n rounds\n",
    "        if topk_W_unchanged >= n:\n",
    "            break\n",
    "    #END MAIN LOOP------------------------------------\n",
    "    #return the best workflow and the k-1 next best workflows/the best workflow's score and the k-1 next best workflows' scores\n",
    "    return (best_W,topk_W),(best_score,topk_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab4b4d-c679-4fa0-9d8c-299e55e784a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install openai --quiet\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1acc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_workflow = Workflow()\n",
    "operators = {'GenerateText':GenerateText} #registered nodes to be used in optimization\n",
    "llm_client = AsyncOpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d42db-cd88-4de1-b2ac-70cab8f94d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\"Summarize the key themes discussed in a dataset of customer reviews about a new smartphone.\", \"The reviews primarily focus on the phone's excellent camera quality, long battery life, and sleek design. However, there are several complaints about the high price and lack of a headphone jack.\"),\n",
    "    \n",
    "    (\"Perform sentiment analysis on the following dataset of tweets related to a recent product launch: 'This product is amazing, totally worth the money!', 'Disappointed with the quality. Expected more for the price.', 'Solid performance, but a bit overpriced.'\", \"The sentiments expressed in the dataset are mixed: one tweet is positive, one is negative, and the third expresses neutral sentiment about the product's price.\"),\n",
    "    \n",
    "    (\"Translate the following French sentence into English: 'Le ciel est bleu.'\", \"The translation of 'Le ciel est bleu' is 'The sky is blue.'\"),\n",
    "    \n",
    "    (\"In a dataset of email conversations, identify if the following text is spam: 'Congratulations! You’ve won a free trip to the Bahamas. Click here to claim your prize.'\", \"The text is classified as spam based on the pattern of congratulatory messages and a suspicious link.\"),\n",
    "    \n",
    "    (\"Extract all names of people from the following sentence: 'John and Mary went to Paris for a vacation.'\", \"The extracted names are: John, Mary.\"),\n",
    "    \n",
    "    (\"Given a dataset of news articles, answer the following question: 'Who is the current president of the United States?' from the text: 'In 2024, Joe Biden is serving his second term as president.'\", \"The current president of the United States is Joe Biden.\"),\n",
    "    \n",
    "    # New examples\n",
    "    (\"Identify the sentiment of the following review: 'The hotel was beautiful but the service was terrible.'\", \"The sentiment is mixed, with positive feedback on the hotel itself but negative feedback regarding the service.\"),\n",
    "    \n",
    "    (\"Given a dataset of product descriptions, extract the key features from this description: 'This laptop features a 15.6-inch display, Intel i7 processor, 16GB RAM, and a 512GB SSD.'\", \"The key features are: 15.6-inch display, Intel i7 processor, 16GB RAM, 512GB SSD.\"),\n",
    "    \n",
    "    (\"Translate the following Spanish sentence into English: 'Me gusta mucho la comida mexicana.'\", \"The translation of 'Me gusta mucho la comida mexicana' is 'I really like Mexican food.'\"),\n",
    "    \n",
    "    (\"Classify the following text as either a question or a statement: 'What time does the movie start?'\", \"The text is classified as a question.\"),\n",
    "    \n",
    "    (\"Based on the dataset of historical weather reports, answer the following: 'What was the highest temperature recorded in July 2020?'\", \"The highest temperature recorded in July 2020 was 104°F.\"),\n",
    "    \n",
    "    (\"In a dataset of product reviews, identify if the following review is fake or genuine: 'This product is absolutely amazing and changed my life completely! I recommend it to everyone!'\", \"The review is likely fake due to its exaggerated and overly positive language.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fd917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_star = await get_optimal_workflow(initial_workflow,evaluate,dataset,operators,llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c9014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workflow_test = Workflow()\n",
    "workflow_test.add_node(GenerateText,'The GenerateText node is designed to create textual responses based on the input it receives. It can synthesize information, provide explanations, or generate creative content depending on the context of the user query. This node will enhance the workflow by ensuring that the responses are not only relevant but also articulated in a coherent and engaging manner.', \n",
    "                       \"Based on the user query, generate a detailed and informative text response that addresses the user's needs and provides additional context or examples where necessary.\",\n",
    "                      'Add a new GenerateText to the workflow')\n",
    "print(workflow_test.extract_chain_info())\n",
    "execute_workflow_over_data(workflow_test,evaluate,dataset,llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f86204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c85668e",
   "metadata": {},
   "source": [
    "### Future Enhancements\n",
    "We can optimize one layer/level of operators.\n",
    "\n",
    "What about two layers of operators? (e.g. operators nested in operators)\n",
    "\n",
    "What about N layers of operators?\n",
    "\n",
    "\n",
    "e.g. can we implement hierarchical optimization so that each and every step in the chain is tightly fit to purpose\n",
    "\n",
    "e.g. can we construct operators from smaller units on the fly?\n",
    "\n",
    "e.g. are there general operators such that these operators can be constructed of these operators at any level of depth\n",
    "\n",
    "\n",
    "We have implemented breadth (operator columns) - can we implement depth (operator rows)?\n",
    "\n",
    "Can we do it just as fast as breadth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
