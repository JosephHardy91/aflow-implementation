{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffac8e67",
   "metadata": {},
   "source": [
    "# ${Aflow}$ Implementation experiments\n",
    "\n",
    "## Source\n",
    "* https://arxiv.org/pdf/2410.10762\n",
    "\n",
    "## Notes\n",
    "* Use gpt-4o-mini as executor/workflow constructor (executor/optimizer)\n",
    "* backprop through modifications (?) that are descriptions for the executor to implement onto the parent\n",
    "     * therefore need a way to modify the parent via text\n",
    "\n",
    "## Algorithm\n",
    "* Split data into 20% val/80% training\n",
    "* $async$ Get scores for initial workflow on val data\n",
    "* Pare down val data to examples with high score variance (above threshold)\n",
    "* set $experiences = []$\n",
    "* set $allresults = []$\n",
    "* set $topkW = []$\n",
    "* set $W^* = None$\n",
    "* set $topkScores = []$\n",
    "* set $bestScore = 0$\n",
    "\n",
    "* for each training step,\n",
    "    * set parent as initial workflow for first step or select parent from last training step results using subroutine ${SelectParent}$\n",
    "    * get the optimizer context (parent workflow + last step experiences)\n",
    "    * use the optimizer with the context and set of available operators to improve the parent workflow and generate $W_{round}$ and the ${modification}$ it's based on\n",
    "    * set $evalresults=[]$\n",
    "    * $async$ for each of 5 rounds\n",
    "        * execute $W_{round}$ on the val data and get ${score}/{cost}$ from evaluator ${E} / {G}$\n",
    "        * append the ${score},{cost}$ to $evalresults$\n",
    "    * append $evalresults$ to $allresults$\n",
    "    * calculate the average $score$ for the 5 rounds from $evalresults$\n",
    "    * create a new experience with the parent workflow, the modification to produce $W_{round}$, and the average $score$\n",
    "    * append the experience to $experiences$ \n",
    "    * if the average $score$ is higher than the $bestScore$, update $W^* = W_{round}$ and $bestScore = $ average $score$\n",
    "    * add $W_{round}$ to $topkW$ and average $score$ to $topkScores$ if $topkW$ has fewer than $k$ elements or average $score$ is greater than the lowest value in $topkScores$\n",
    "    * if $topkScores$ hasn't changed in specified (n) number of rounds, stop early and return $W^*$\n",
    "* return $W^*$\n",
    "    \n",
    "### Algorithm Unknowns\n",
    "* how to get evaluator G/E?\n",
    "* how do initial scores get computed? are there multiple rounds of scoring per example in order to generate variance samples?\n",
    "* how does the optimizer generate new workflows from previous samples (experiences)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b28c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "from random import choices,seed,random\n",
    "from uuid import uuid5 as uuid\n",
    "from uuid import NAMESPACE_DNS\n",
    "from copy import deepcopy\n",
    "from pydantic import BaseModel,conint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b965f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DONE: Implement create_experience\n",
    "class Experience:\n",
    "    def __init__(self,workflow,round_added,score,parent_score=None,modification=None):\n",
    "        self.workflow = workflow\n",
    "        \n",
    "        self.parent_workflow = self.workflow.parent\n",
    "        if self.parent_workflow:\n",
    "            self.parent_workflow_id = self.parent_workflow.id\n",
    "        else:\n",
    "            self.parent_workflow_id = None\n",
    "        self.parent_score = parent_score\n",
    "        \n",
    "        if modification:\n",
    "            self.round_modification_added = round_added\n",
    "            self.round_added = self.parent_workflow.round_added\n",
    "        else:\n",
    "            self.round_modification_added = None\n",
    "            self.round_added = round_added\n",
    "        \n",
    "        self.score = score\n",
    "        self.modification = modification\n",
    "        \n",
    "        if self.parent_score:\n",
    "            self.success = self.score > self.parent_score\n",
    "        else:\n",
    "            self.success = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Experience Info:\n",
    "        Metadata:\n",
    "        - Workflow with modification ID: {self.workflow.id}\n",
    "        - Workflow with modification score: {self.score}\n",
    "        - Workflow without modification ID: {self.parent_workflow_id}\n",
    "        - Workflow without modification score: {self.parent_score}\n",
    "        \n",
    "        \n",
    "        Modifications:\n",
    "        - Lead to improvement over workflow without modifications: {self.success}\n",
    "        - Round Workflow was modified: {self.round_added}\n",
    "        - Modification details: {self.modification}\n",
    "        \"\"\"\n",
    "\n",
    "# DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "class Experiences:\n",
    "    def __init__(self):\n",
    "        self.experiences = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.experiences[key]\n",
    "        \n",
    "    def create_experience(self,workflow,round_,score,modification=None):\n",
    "        if round_ not in self.experiences:\n",
    "            self.experiences[round_] = {}\n",
    "        parent_score = None\n",
    "        if modification:\n",
    "            parent_score = self.experiences[workflow.parent.round_added][workflow.parent.id].score\n",
    "        self.experiences[round_][workflow.id] = Experience(workflow,round_,parent_score,modification)\n",
    "        return self.experiences[round_][workflow.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4941f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowNode:\n",
    "    def __init__(self,next_node=None):\n",
    "        if next_node is None:\n",
    "            self.next = FinalNode()\n",
    "        else:\n",
    "            self.next = next_node()\n",
    "        \n",
    "    def __call__(self,prev_step):\n",
    "        #does something with prev_step and returns it\n",
    "        ...\n",
    "        \n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "    \n",
    "class FinalNode:\n",
    "    def __init__(self):\n",
    "        self.next = None\n",
    "        \n",
    "    def __call__(self,prev_step):\n",
    "        return prev_step\n",
    "    \n",
    "    def copy(self):\n",
    "        return self\n",
    "    \n",
    "class Workflow:\n",
    "    #contains all of the information needed to understand a single workflow\n",
    "    def __init__(self,round_added = 0,parent = None):\n",
    "        self.next = FinalNode()\n",
    "        self.round_added = round_added\n",
    "        self.copies = 0\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        self.id = uuid(NAMESPACE_DNS,str(self.round_added)+chr(97+self.copies))\n",
    "        self.parent = parent\n",
    "        \n",
    "    def start(self,first_inputs):\n",
    "        self.next(**first_inputs)\n",
    "        \n",
    "    def add_node(self,node_class:WorkflowNode):\n",
    "        current_node = self.next\n",
    "        while not isinstance(current_node,FinalNode):\n",
    "            current_node = current_node.next\n",
    "            \n",
    "        new_node = node_class()\n",
    "        current_node.next = new_node\n",
    "        \n",
    "    def copy(self):\n",
    "        # Create a shallow copy of the current instance\n",
    "        new_workflow = Workflow(self.round_added,self)\n",
    "        new_workflow.next = self.next.copy()\n",
    "        new_workflow.copies = self.copies + 1\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        new_workflow.id = uuid(NAMESPACE_DNS, str(new_workflow.round_added) + chr(97 + new_workflow.copies))\n",
    "        return new_workflow \n",
    "    \n",
    "    def extract_chain_info(self):\n",
    "        chain = []\n",
    "        chain_info = []\n",
    "        current_node = self.next\n",
    "        while current_node.next:\n",
    "            chain.append(type(current_node))\n",
    "            chain_info.append(current_node.description)\n",
    "            \n",
    "        return ' -->> '.join(map(str,chain)),'\\n'.join([':'.join((node,node_info)) for node,node_info in zip(map(str,chain),chain_info)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69d054de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement split_data\n",
    "def split_data(dataset,val_ratio=0.2,random_seed=1337)\n",
    "    seed(random_seed)\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    for datum in dataset:\n",
    "        if random() < val_ratio:\n",
    "            val_set.append(datum)\n",
    "        else:\n",
    "            test_set.append(datum)\n",
    "            \n",
    "    return val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cec8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement determine_variance_threshold\n",
    "# DONE: Implement select_high_variance_instances\n",
    "# TODO: Handle asynchronous running (e.g. each output score set val data i may not correspond with input val data i)\n",
    "def select_high_variance_instances(val_data,score_sets,marginal_variance_tolerance_perc = 0.05):\n",
    "    variances = []\n",
    "    n_data_points = len(score_sets[0])\n",
    "    for i in n_data_points:\n",
    "        scores_i = [score_set[i] for score_set in score_sets]\n",
    "        num_scores_i = len(scores_i)\n",
    "        avg_score_i = sum(scores_i)/num_scores_i\n",
    "        sample_variance_i = (1/num_scores_i)*sum((score_i-avg_score_i)**2 for score_i in scores_i)\n",
    "        variances.append(sample_variance_i)\n",
    "        \n",
    "    sorted_variances = sorted(list(set(variances)),reversed=True)\n",
    "    #use elbow method diminishing gains based on marginal_variance_tolerance_perc\n",
    "    last_variance = None\n",
    "    for variance in sorted_variances:\n",
    "        if last_variance is None:\n",
    "            last_variance = variance\n",
    "        else:\n",
    "            perc_change = (variance/last_variance)-1\n",
    "            if perc_change < marginal_variance_tolerance_perc:\n",
    "                return last_variance\n",
    "            last_variance = variance\n",
    "            \n",
    "    return [datum for i,datum in enumerate(val_data) if variances[i]>last_variance],last_variance\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e3b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement select_parent\n",
    "def select_parent(topk_scores,topk_W):\n",
    "    probabilities = calculate_mixed_probabilities(topk_scores) #DONE: Implement calculate_mixed_probabilities\n",
    "    return sample_from_categorical(probabilities,topk_W) #DONE: Implement sample_from_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d73c0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: Implement calculate_mixed_probabilities\n",
    "def calculate_mixed_probabilities(scores,lambda_=0.4,alpha=0.5): #TODO: Determine appropriate values of lambda and alpha from the paper\n",
    "    n = len(scores)\n",
    "    max_score = max(scores)\n",
    "    w = (e**(alpha*(s_i-max_score)) for s_i in scores)\n",
    "    P_score = (w_i/sum(w) for w_i in w)\n",
    "    P_uniform = (1/n for _ in range(n))\n",
    "    P_mixed = (lambda_ * P_uniform_i + (1-lambda_) * P_score_i for P_score_i,P_uniform_i in zip(P_score,P_uniform))\n",
    "    return P_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85f425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: Implement sample_from_categorical\n",
    "def sample_from_categorical(probabilities,workflows):\n",
    "    return random.choices(population=workflows,weights=probabilities,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "263d4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement load_context\n",
    "def load_context(parent_workflow,experiences):\n",
    "    #get the experiences related to parent_workflow\n",
    "    #parent_workflow may or may not have modifications\n",
    "    experience_info = []\n",
    "    for round_ in experiences:\n",
    "        if parent_workflow.id in experiences[round_]:\n",
    "            experience_info.append(str(experiences[round_][parent_workflow.id]))\n",
    "    workflow_experiences = '\\n\\n'.join(experience_info)\n",
    "    \n",
    "    #get the node chain visualization and node descriptions parent_workflow implements\n",
    "    chain_viz,chain_descriptions = parent_workflow.extract_chain_info()\n",
    "    \n",
    "    context_string = f\"\"\"\n",
    "        Context for workflow {parent_workflow.id}:\n",
    "        \n",
    "        Workflow node chain visualization:\n",
    "        {chain_viz}\n",
    "        \n",
    "        Descriptions of each node:\n",
    "        {chain_descriptions}\n",
    "        \n",
    "        Trials (experiences) involving this workflow:\n",
    "        {workflow_experiences}\n",
    "    \"\"\"\n",
    "    return context_string\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa6c480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement optimize\n",
    "class NodeAdd(BaseModel):\n",
    "    node:str\n",
    "    modification:str\n",
    "    \n",
    "def optimize(parent_workflow,context_string,dict_of_valid_operations,llm_client):\n",
    "    C,O = context_string,'\\n\\n'.join(map(str,dict_of_valid_operations.keys())) #TODO: Implement Node __str__ for this purpose\n",
    "    \n",
    "    optimize_system_prompt = \"\"\"\n",
    "    You are being given a chance to improve the workflow used to answer user queries.\n",
    "    You will be given information about the workflows as well as information about, and results of, previous attempts (if applicable) to improve the workflow.\n",
    "    You will also be given information about valid operations that can be added to the workflow in order to increase its ability to successfully answer the user query.\n",
    "    \n",
    "    You will be expected to reason step-by-step in order to come to a conclusion about what node (valid operation) to add to the workflow.\n",
    "    \n",
    "    At the end of your reasoning steps, you should output a single json object in the following format:\n",
    "    {\"node\":\"<NAME OF NODE>\",\"modification\":\"Add a new <NAME OF NODE> to <AND SO ON>\"}\n",
    "    where \"<NAME OF NODE>\" is one of given node names. If you fail to choose an existing node name, you will fail your task.\n",
    "    And where <AND SO ON> is where the rest of your explanation should go. If you fail to rationally, simply and clearly describe \n",
    "    your reasoning here, you will fail your task and future optimizations will go poorly.\n",
    "    \n",
    "    You are OPTIMIZER.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimize_user_prompt = f\"\"\"\n",
    "    Here is your context:\n",
    "    Workflow information:\n",
    "    {C}\n",
    "    \n",
    "    Valid operations/nodes:\n",
    "    {O}\n",
    "    \"\"\"\n",
    "    \n",
    "    client_response = llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":optimize_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":optimize_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o-mini'\n",
    "        response_format=NodeAdd,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    \n",
    "    nodeadd_obj = client_response.choices[0].message.parsed\n",
    "    W_round = parent_workflow.copy().add_node(dict_of_valid_operations(nodeadd_obj['node']))\n",
    "    return W_round,nodeadd_obj['modification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: Implement evaluator \n",
    "#will use gpt-4o to assess results (small dataset for now)\n",
    "class Evaluation(BaseModel):\n",
    "    grade:conint(ge=0,le=10)\n",
    "        \n",
    "def evaluate(question,correct_answer,given_answer):\n",
    "    evaluate_system_prompt = \"\"\"\n",
    "    You are being given a chance to evaluate the result of a workflow used to answer user queries.\n",
    "    You will be given the user query, the correct answer, the the answer provided by the workflow.\n",
    "    \n",
    "    You will be expected to succinctly explain any issues with the answer from a content and understandability perspective.\n",
    "    \n",
    "    At the end of your succinct explanation, you should output a single json object in the following format:\n",
    "    {\"grade\":<GRADE>}\n",
    "    where <GRADE> is an integer between 0 and 10, where 0 is completely irrelevant and/or completely unintelligible, and 10 is perfect in both content and understandability. In between is a continuum of integer grades describing differing levels of imperfection. \n",
    "    \n",
    "    You are EVALUATOR.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_user_prompt = f\"\"\"\n",
    "    Here is the user query:\n",
    "    {question}\n",
    "    \n",
    "    Here is the correct answer:\n",
    "    {correct_answer}\n",
    "    \n",
    "    Here is the answer generated by the workflow:\n",
    "    {given_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    client_response = llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":evaluate_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":evaluate_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o'\n",
    "        response_format=Evaluation,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    \n",
    "    evaluation_obj = client_response.choices[0].message.parsed\n",
    "    return evaluation_obj.grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3b41c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Implement execute_workflow_over_data\n",
    "# TODO: Make this asynchronous\n",
    "# TODO: Implement categorical loss\n",
    "def execute_workflow_over_data(workflow,evaluator,data,llm_client):\n",
    "    scores = []\n",
    "    costs = []\n",
    "    for datum in data:\n",
    "        q,a = datum\n",
    "        a_hat,cost = workflow.start({'data':q,'llm_client':llm_client})\n",
    "        score = evaluator(q,a,a_hat) #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "        scores.append(score)\n",
    "        costs.append(cost)\n",
    "    return scores,sum(costs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68ebce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_workflow(initial_workflow,evaluator,dataset,operators,llm_client,number_of_rounds=20,k=3,early_stopping_rounds=5,scoring_rounds=5):\n",
    "    #DONE: Implement evaluator\n",
    "    #Initialize variables\n",
    "    W_0, G, D, N, O, k, n, I = initial_workflow,evaluator,dataset,number_of_rounds,operators,k, \\\n",
    "                                early_stopping_rounds,scoring_rounds\n",
    "    results = []\n",
    "    experiences = Experiences()\n",
    "    topk_W = []\n",
    "    best_W = None\n",
    "    topk_scores = []\n",
    "    best_score = 0\n",
    "    topk_W_unchanged = 0\n",
    "    \n",
    "    #Split dataset\n",
    "    Dv,Dt = split_data(dataset) #DONE: Implement split_data\n",
    "    \n",
    "    #Gather initial scores to determine final validation dataset\n",
    "    score_sets,_ = zip(*[execute_workflow_over_data(W_0,G,Dv) for _ in range(k)]) #DONE: Implement execute_workflow_over_data\n",
    "    Dv,variance_threshold = select_high_variance_instances(Dv,score_sets)   #DONE: Implement select_high_variance_instances\n",
    "                                                                        #DONE: Implement determine_variance_threshold\n",
    "    \n",
    "    print(f'{len(Dv)} validation samples selected based on variance threshold {variance_threshold:.2f}')\n",
    "    #MAIN LOOP----------------------------------------\n",
    "    #Iterate to improve bestScore\n",
    "    for round_ in range(len(N)):\n",
    "        if round_ == 1:\n",
    "            parent = W_0\n",
    "        else:\n",
    "            parent = select_parent(topk_scores,topk_W) #DONE: Implement select_parent\n",
    "        \n",
    "        #Load context for parent and perform optimization forward pass\n",
    "        context = load_context(parent,experiences) #DONE: Implement load_context\n",
    "        W_round, modification = optimize(parent,context,O,llm_client) #DONE: Implement optimize\n",
    "        \n",
    "        #Generate validation scores for modified workflow to demonstrate relative performance\n",
    "        for i in range(I):\n",
    "            scores,cost = execute_workflow_over_data(W_round,G,Dv,llm_client) #DONE: Implement execute_workflow_over_data\n",
    "            round_score = sum(scores)/len(scores) #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "            results.append((round_,round_score,cost))\n",
    "        \n",
    "        #Capture a new experience for use in future optimization passes by using the modified workflow validation scores as gain\n",
    "        average_score = sum(r[1] for r in results if results[0]==round_)/len(r[1] for r in results if results[0]==round_)\n",
    "        experience = experiences.create_experience(W_round,round_,average_score,modification) #DONE: Implement create_experience\n",
    "#         experiences.append(experience) #DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "        \n",
    "        #Save the previous topk_W for the later early-stopping check\n",
    "        prev_topk_W = topk_W\n",
    "        \n",
    "        #Update best W, if applicable this round\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_W = W_round\n",
    "            \n",
    "            topk_W.append(W_round)\n",
    "            topk_scores.append(average_score)\n",
    "        \n",
    "        #Try to include this round's W_round into topk_W if the W_round's score is within the top k scores\n",
    "        combined = sorted(zip(topk_scores,topk_W),reverse=True)\n",
    "        topk_scores[:],topk_W[:] = zip(*combined)\n",
    "        \n",
    "        #Boot any workflows outside of the top k scores\n",
    "        if len(topk_scores)>k:\n",
    "            topk_scores = list(topk_scores[:-1])\n",
    "            topk_W = list(topk_W[:-1])\n",
    "            \n",
    "        #Check if topk_W is the same as the prev topk_W and increment the early stopping counter if so. If not, set counter to 0.\n",
    "        if set([W.id for W in topk_W])==set([prevW.id for prevW in prev_topk_W]):\n",
    "            topk_W_unchanged+=1 #DONE: make sure workflows are being compared based on their IDs \n",
    "                                                    #(since there may not be consistent parity between naive objects)\n",
    "        else:\n",
    "            topk_W_unchanged = 0\n",
    "        \n",
    "        #Stop the workflow improvement early if no new workflows have joined the top k workflows in n rounds\n",
    "        if topk_W_unchanged >= n:\n",
    "            break\n",
    "    #END MAIN LOOP------------------------------------\n",
    "    #return the best workflow and the k-1 next best workflows/the best workflow's score and the k-1 next best workflows' scores\n",
    "    return (best_W,topk_W),(best_score,topk_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e1acc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_workflow = Workflow()\n",
    "operators = [] #registered nodes to be used in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd8fd917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': <__main__.FinalNode at 0x264ff72f888>,\n",
       " 'round_added': 0,\n",
       " 'copies': 0,\n",
       " 'id': UUID('81060c4f-efaf-54d6-b722-5509dce87911')}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_workflow.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "303c9014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': <__main__.FinalNode at 0x264ff72f888>,\n",
       " 'round_added': 0,\n",
       " 'copies': 1,\n",
       " 'id': UUID('4cf211f1-3335-5c59-a280-9ad90e75b23e')}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_workflow.copy().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f86204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c85668e",
   "metadata": {},
   "source": [
    "### Future Enhancements\n",
    "We can optimize one layer/level of operators.\n",
    "\n",
    "What about two layers of operators? (e.g. operators nested in operators)\n",
    "\n",
    "What about N layers of operators?\n",
    "\n",
    "\n",
    "e.g. can we implement hierarchical optimization so that each and every step in the chain is tightly fit to purpose\n",
    "\n",
    "e.g. can we construct operators from smaller units on the fly?\n",
    "\n",
    "e.g. are there general operators such that these operators can be constructed of these operators at any level of depth\n",
    "\n",
    "\n",
    "We have implemented breadth (operator columns) - can we implement depth (operator rows)?\n",
    "\n",
    "Can we do it just as fast as breadth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
