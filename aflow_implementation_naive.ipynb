{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffac8e67",
   "metadata": {},
   "source": [
    "# ${Aflow}$ Implementation experiments\n",
    "\n",
    "## Source\n",
    "* https://arxiv.org/pdf/2410.10762\n",
    "\n",
    "## Notes\n",
    "* Use gpt-4o-mini as executor/workflow constructor (executor/optimizer)\n",
    "* backprop through modifications (?) that are descriptions for the executor to implement onto the parent\n",
    "     * therefore need a way to modify the parent via text\n",
    "\n",
    "## Algorithm\n",
    "* Split data into 20% val/80% training\n",
    "* $async$ Get scores for initial workflow on val data\n",
    "* Pare down val data to examples with high score variance (above threshold)\n",
    "* set $experiences = []$\n",
    "* set $allresults = []$\n",
    "* set $topkW = []$\n",
    "* set $W^* = None$\n",
    "* set $topkScores = []$\n",
    "* set $bestScore = 0$\n",
    "\n",
    "* for each training step,\n",
    "    * set parent as initial workflow for first step or select parent from last training step results using subroutine ${SelectParent}$\n",
    "    * get the optimizer context (parent workflow + last step experiences)\n",
    "    * use the optimizer with the context and set of available operators to improve the parent workflow and generate $W_{round}$ and the ${modification}$ it's based on\n",
    "    * set $evalresults=[]$\n",
    "    * $async$ for each of 5 rounds\n",
    "        * execute $W_{round}$ on the val data and get ${score}/{cost}$ from evaluator ${E} / {G}$\n",
    "        * append the ${score},{cost}$ to $evalresults$\n",
    "    * append $evalresults$ to $allresults$\n",
    "    * calculate the average $score$ for the 5 rounds from $evalresults$\n",
    "    * create a new experience with the parent workflow, the modification to produce $W_{round}$, and the average $score$\n",
    "    * append the experience to $experiences$ \n",
    "    * if the average $score$ is higher than the $bestScore$, update $W^* = W_{round}$ and $bestScore = $ average $score$\n",
    "    * add $W_{round}$ to $topkW$ and average $score$ to $topkScores$ if $topkW$ has fewer than $k$ elements or average $score$ is greater than the lowest value in $topkScores$\n",
    "    * if $topkScores$ hasn't changed in specified (n) number of rounds, stop early and return $W^*$\n",
    "* return $W^*$\n",
    "    \n",
    "### Algorithm Unknowns\n",
    "* how to get evaluator G/E?\n",
    "* how do initial scores get computed? are there multiple rounds of scoring per example in order to generate variance samples?\n",
    "* how does the optimizer generate new workflows from previous samples (experiences)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b28c83b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import e\n",
    "from random import choices,seed,random\n",
    "from uuid import uuid5 as uuid\n",
    "from uuid import NAMESPACE_DNS\n",
    "from copy import deepcopy\n",
    "from pydantic import BaseModel,conint\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b965f9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# DONE: Implement create_experience\n",
    "class Experience:\n",
    "    def __init__(self,workflow,round_added,score,parent_score=None,modification=None):\n",
    "        self.workflow = workflow\n",
    "        \n",
    "        self.parent_workflow = self.workflow.parent\n",
    "        if self.parent_workflow:\n",
    "            self.parent_workflow_id = self.parent_workflow.id\n",
    "        else:\n",
    "            self.parent_workflow_id = None\n",
    "        self.parent_score = parent_score\n",
    "        \n",
    "        if modification:\n",
    "            self.round_modification_added = round_added\n",
    "            self.round_added = self.parent_workflow.round_added\n",
    "        else:\n",
    "            self.round_modification_added = None\n",
    "            self.round_added = round_added\n",
    "        \n",
    "        self.score = score\n",
    "        self.modification = modification\n",
    "        \n",
    "        if self.parent_score and (isinstance(self.parent_score,float) or isinstance(self.parent_score,int)):\n",
    "            self.success = self.score > self.parent_score\n",
    "        else:\n",
    "            self.success = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Experience Info:\n",
    "        Metadata:\n",
    "        - Workflow with modification ID: {self.workflow.id}\n",
    "        - Workflow with modification score: {self.score}\n",
    "        - Workflow without modification ID: {self.parent_workflow_id}\n",
    "        - Workflow without modification score: {self.parent_score}\n",
    "        \n",
    "        \n",
    "        Modification that lead to this experience:\n",
    "        - Did the modification lead to improvement over workflow without modifications: {self.success}\n",
    "        - Round Workflow was modified: {self.round_added}\n",
    "        - Modification details: {self.modification}\n",
    "        \"\"\"\n",
    "\n",
    "# DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "class Experiences:\n",
    "    def __init__(self):\n",
    "        self.experiences = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.experiences[key]\n",
    "        \n",
    "    def create_experience(self,workflow,round_,score,modification=None):\n",
    "        if round_ not in self.experiences:\n",
    "            self.experiences[round_] = {}\n",
    "        parent_score = None\n",
    "        if modification:\n",
    "            #print(self.experiences,round_,workflow.parent.round_added,score,modification)\n",
    "            parent_score = self.experiences[workflow.parent.round_added][workflow.parent.id].score\n",
    "        self.experiences[round_][workflow.id] = Experience(workflow,round_,score,parent_score,modification)\n",
    "        return self.experiences[round_][workflow.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4941f12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WorkflowNode:\n",
    "    def __init__(self,description,prompt,modification,next_node=None):\n",
    "        self.description = description\n",
    "        self.prompt = prompt\n",
    "        self.modification = modification\n",
    "        if next_node is None:\n",
    "            self.next = FinalNode()\n",
    "        else:\n",
    "            self.next = next_node()\n",
    "        \n",
    "    def __call__(self,**kwargs):\n",
    "        #does something with prev_step and returns it\n",
    "        llm_client = kwargs['llm_client']\n",
    "        data = kwargs['data']\n",
    "        \n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "    \n",
    "class FinalNode:\n",
    "    def __init__(self):\n",
    "        self.next = None\n",
    "        self.description = 'This node is a final sentinel and simply passes what it\\'s given through.'\n",
    "        \n",
    "    async def __call__(self,**kwargs):\n",
    "        #print(kwargs)\n",
    "        return kwargs\n",
    "    \n",
    "    def copy(self):\n",
    "        return self\n",
    "    \n",
    "class Workflow:\n",
    "    #contains all of the information needed to understand a single workflow\n",
    "    def __init__(self,round_added = 0,parent = None):\n",
    "        self.next = FinalNode()\n",
    "        self.round_added = round_added\n",
    "        self.copies = 0\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        self.id = str(uuid(NAMESPACE_DNS,str(self.round_added)+chr(97+self.copies)))\n",
    "        self.parent = parent\n",
    "        \n",
    "    async def start(self,first_inputs):\n",
    "        return await self.next(**first_inputs)\n",
    "        \n",
    "    def add_node(self,node_class:WorkflowNode,description,prompt,modification):\n",
    "        new_node = node_class(description,prompt,modification)\n",
    "        \n",
    "        if not isinstance(self.next,FinalNode):\n",
    "            current_node = self.next\n",
    "            while not isinstance(current_node.next,FinalNode):\n",
    "                current_node = current_node.next\n",
    "            #print(type(current_node))\n",
    "        else:\n",
    "            current_node = self\n",
    "        \n",
    "        current_node.next = new_node\n",
    "        return self\n",
    "        \n",
    "    def copy(self, round_):\n",
    "        # Create a shallow copy of the current instance\n",
    "        new_workflow = Workflow(round_,self)\n",
    "        new_workflow.next = self.next.copy()\n",
    "        new_workflow.copies = self.copies + 1\n",
    "        # DONE: make sure workflows are being compared based on their IDs (since there may not be consistent parity between naive objects)\n",
    "        new_workflow.id = str(uuid(NAMESPACE_DNS, str(new_workflow.round_added) + chr(97 + new_workflow.copies)))\n",
    "        return new_workflow \n",
    "\n",
    "    def __lt__(self,other):\n",
    "        return self.copies < other.copies\n",
    "    \n",
    "    def extract_chain_info(self):\n",
    "        chain = []\n",
    "        chain_info = []\n",
    "        current_node = self.next\n",
    "        while current_node.next:\n",
    "            chain.append(type(current_node))\n",
    "            chain_info.append(current_node.description)\n",
    "            current_node = current_node.next\n",
    "            \n",
    "        return ' -->> '.join(map(str,chain)),'\\n'.join([':'.join((node,node_info)) for node,node_info in zip(map(str,chain),chain_info)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f910299-a82e-4e63-8d00-a5c335ae64b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#available operators\n",
    "# class GeneratePrompts(WorkflowNode):\n",
    "#     def __init__():\n",
    "#         super().__init__()\n",
    "#     def __call__(self,**kwargs):\n",
    "#         #does something with prev_step and returns it\n",
    "#         llm_client = kwargs['llm_client']\n",
    "#         data = kwargs['data']\n",
    "#         prompt\n",
    "#         results = []\n",
    "#         for instance in data:\n",
    "#             llm_client.chat.completions.create(\n",
    "#                 messages=[\n",
    "#                     messages+[\n",
    "#                         {\"role\":\"user\"},\n",
    "#                         {\"content\":instance}\n",
    "#                     ]\n",
    "#                 ],\n",
    "#                 model='gpt-4o-mini'\n",
    "#             ).messages[0].content\n",
    "#         self.next(messages=results,data=data,\n",
    "#                   llm_client=llm_client)        \n",
    "#     def copy(self):\n",
    "#         return deepcopy(self)\n",
    "    \n",
    "class GenerateText(WorkflowNode):\n",
    "    def __init__(self,description,prompt,modification):\n",
    "        super().__init__(description,prompt,modification)\n",
    "        \n",
    "    async def __call__(self,**kwargs):\n",
    "        #does something with prev_step and returns it\n",
    "        llm_client = kwargs.get('llm_client',None)\n",
    "        data = kwargs.get('data',[])\n",
    "        results = []\n",
    "        #print('in node:',llm_client,';',data)\n",
    "        for instance in data:\n",
    "            result = (await llm_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":self.prompt},\n",
    "                    {\"role\":\"user\",\"content\":instance}\n",
    "                ],\n",
    "                model='gpt-4o-mini'\n",
    "            )).choices[0].message.content\n",
    "            results.append(result)\n",
    "        return await self.next(data=results,llm_client=llm_client)\n",
    "        \n",
    "    def copy(self):\n",
    "        return deepcopy(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8e6bd-57d9-4a02-ba7c-5079f0d60d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d054de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement split_data\n",
    "def split_data(dataset,val_ratio=0.2,random_seed=1337):\n",
    "    seed(random_seed)\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    for datum in dataset:\n",
    "        if random() < val_ratio:\n",
    "            val_set.append(datum)\n",
    "        else:\n",
    "            test_set.append(datum)\n",
    "            \n",
    "    return val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cec8c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement determine_variance_threshold\n",
    "# DONE: Implement select_high_variance_instances\n",
    "# TODO: Handle asynchronous running (e.g. each output score set val data i may not correspond with input val data i)\n",
    "def select_high_variance_instances(val_data,score_sets,marginal_variance_tolerance_perc = 0.05):\n",
    "    variances = []\n",
    "    n_data_points = len(score_sets[0])\n",
    "    for i in range(n_data_points):\n",
    "        scores_i = [score_set[i] for score_set in score_sets]\n",
    "        num_scores_i = len(scores_i)\n",
    "        avg_score_i = sum(scores_i)/num_scores_i\n",
    "        sample_variance_i = (1/num_scores_i)*sum((score_i-avg_score_i)**2 for score_i in scores_i)\n",
    "        variances.append(sample_variance_i)\n",
    "        \n",
    "    sorted_variances = sorted(list(set(variances)),reverse=True)\n",
    "    #use elbow method diminishing gains based on marginal_variance_tolerance_perc\n",
    "    last_variance = None\n",
    "    for variance in sorted_variances:\n",
    "        if last_variance is None:\n",
    "            last_variance = variance\n",
    "        else:\n",
    "            perc_change = (variance/last_variance)-1\n",
    "            if perc_change < marginal_variance_tolerance_perc:\n",
    "                return last_variance\n",
    "            last_variance = variance\n",
    "            \n",
    "    return [datum for i,datum in enumerate(val_data) if variances[i]>last_variance],last_variance\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e3b635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement select_parent\n",
    "def select_parent(topk_scores,topk_W):\n",
    "    probabilities = calculate_mixed_probabilities(topk_scores) #DONE: Implement calculate_mixed_probabilities\n",
    "    return sample_from_categorical(probabilities,topk_W) #DONE: Implement sample_from_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73c0bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement calculate_mixed_probabilities\n",
    "def calculate_mixed_probabilities(scores,lambda_=0.4,alpha=0.5): #TODO: Determine appropriate values of lambda and alpha from the paper\n",
    "    n = len(scores)\n",
    "    max_score = max(scores)\n",
    "    w = list(e**(alpha*(s_i-max_score)) for s_i in scores)\n",
    "    P_score = (w_i/sum(w) for w_i in w)\n",
    "    P_uniform = (1/n for _ in range(n))\n",
    "    P_mixed = (lambda_ * P_uniform_i + (1-lambda_) * P_score_i for P_score_i,P_uniform_i in zip(P_score,P_uniform))\n",
    "    return P_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f425c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement sample_from_categorical\n",
    "def sample_from_categorical(probabilities,workflows):\n",
    "    return choices(population=workflows,weights=probabilities,k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238d24de-28ac-4e17-9c8f-b28674131ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list(calculate_mixed_probabilities([0.33,0.233]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263d4239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement load_context\n",
    "def load_context(parent_workflow,experiences):\n",
    "    #get the experiences related to parent_workflow\n",
    "    #parent_workflow may or may not have modifications\n",
    "    experience_info = []\n",
    "    for round_ in experiences:\n",
    "        if parent_workflow.id in experiences[round_]:\n",
    "            experience_info.append(str(experiences[round_][parent_workflow.id]))\n",
    "    workflow_experiences = '\\n\\n'.join(experience_info)\n",
    "    \n",
    "    #get the node chain visualization and node descriptions parent_workflow implements\n",
    "    chain_viz,chain_descriptions = parent_workflow.extract_chain_info()\n",
    "    \n",
    "    context_string = f\"\"\"\n",
    "        Context for workflow {parent_workflow.id}:\n",
    "        \n",
    "        Workflow node chain visualization:\n",
    "        {chain_viz}\n",
    "        \n",
    "        Descriptions of each node:\n",
    "        {chain_descriptions}\n",
    "        \n",
    "        Trials (experiences) involving this workflow:\n",
    "        {workflow_experiences}\n",
    "    \"\"\"\n",
    "    return context_string\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6c480c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement optimize\n",
    "class NodeAdd(BaseModel):\n",
    "    node:str\n",
    "    modification:str\n",
    "    description:str\n",
    "    prompt:str\n",
    "    \n",
    "async def optimize(round_,parent_workflow,context_string,dict_of_valid_operations,llm_client,reward_delay, parent_maturity):\n",
    "    C,O = context_string,'\\n\\n'.join(map(str,dict_of_valid_operations.keys())) #TODO: Implement Node __str__ for this purpose\n",
    "    \n",
    "    optimize_system_prompt = f\"\"\"\n",
    "    You are being given a chance to improve the workflow used to answer user queries.\n",
    "    You will be given information about the workflows as well as information about, and results of, previous attempts (if applicable) to improve the workflow.\n",
    "    You will also be given information about valid operations that can be added to the workflow in order to increase its ability to successfully answer the user query.\n",
    "    \n",
    "    You will be expected to reason step-by-step in order to come to a conclusion about what node (valid operation) to add to the workflow and the prompt that should direct it.\n",
    "    \n",
    "    At the end of your reasoning steps, you should output a single json object in the following format:\n",
    "    {{\"node\":\"<NAME OF NODE>\",\"modification\":\"Add a new <NAME OF NODE> to <AND SO ON>\",\"description\":\"<DESCRIPTION>\",\"prompt:\"<PROMPT>\"}}\n",
    "    where \"<NAME OF NODE>\" is one of given node names. If you fail to choose an existing node name, you will fail your task.\n",
    "    And where <AND SO ON> is where the rest of your explanation should go. If you fail to rationally, simply and clearly describe \n",
    "    your reasoning here, you will fail your task and future optimizations will go poorly.\n",
    "    Third, <DESCRIPTION> is your description of the node's purpose and how it is intended to go about its purpose. This is critical for communicating to yourself in the future about why you created a certain node, so be detailed.\n",
    "    And, finally, <PROMPT>, which is where you exercise your best ability to optimize and improve the results of the next node. If you fail to rationally, simply, and clearly describe\n",
    "    how the next node should interact with its data (which follows from the last node in the current workflow), the optimization task will fail utterly.\n",
    "\n",
    "    Although your communication should be clear, you are allowed to use your resources creatively. For example, you might specify future node expansion if-then instructions in the descriptions,\n",
    "    and/or you may instruct the model to keep part of the previous prompt in the answer so that it's available to later nodes.\n",
    "\n",
    "    Be creative and err on the side of thinking out loud and saying something that isn't useful or doesn't get used rather than not having the idea at all.\n",
    "\n",
    "    In addition, when the node is \"GenerateText\", you are allowed to use the node to generate any output you want, as long as it is within the limits of what is producible by an LLM alone.\n",
    "\n",
    "    Although you may expect your changes to be evaluated at least {reward_delay} times (henceforth referred to as the reward delay) without critical failure, \n",
    "    unless you use nodes that capitalize on the information gained thus far, you will not receive a good evaluation, since your primary goal is to make the workflow fit for responding well within the system. \n",
    "\n",
    "    You have lived through {parent_maturity} evaluations and will die in {max(reward_delay-parent_maturity,0)} evaluations unless you improve.\n",
    "    \n",
    "    You are BRILLIANT OPTIMIZER.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimize_user_prompt = f\"\"\"\n",
    "    Here is your context:\n",
    "    Workflow information:\n",
    "    {C}\n",
    "    \n",
    "    Valid operations/nodes:\n",
    "    {O}\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(optimize_user_prompt)\n",
    "    \n",
    "    client_response = await llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":optimize_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":optimize_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o-mini',\n",
    "        response_format=NodeAdd,\n",
    "        temperature = 1.2\n",
    "    )\n",
    "    \n",
    "    nodeadd_obj = client_response.choices[0].message.parsed\n",
    "    \n",
    "    #print(nodeadd_obj.dict())\n",
    "    \n",
    "    W_round = parent_workflow.copy(round_).add_node(dict_of_valid_operations.get(nodeadd_obj.node),nodeadd_obj.description,nodeadd_obj.prompt,nodeadd_obj.modification)\n",
    "    return W_round,nodeadd_obj.modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c84e5a8-fc1d-4fa5-a0b8-136234bf7321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# na = NodeAdd(node = '1',description = '2',prompt = '3',modification = '4')\n",
    "# # na.node = '1'\n",
    "# # na.description = '2'\n",
    "# # na.prompt = '3'\n",
    "# # na.modification = '4'\n",
    "# na.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba8da5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONE: Implement evaluator \n",
    "#will use gpt-4o to assess results (small dataset for now)\n",
    "class Evaluation(BaseModel):\n",
    "    grade:int#conint(ge=0,le=10)\n",
    "        \n",
    "async def evaluate(question,correct_answer,given_answer):\n",
    "    evaluate_system_prompt = \"\"\"\n",
    "    You are being given a chance to evaluate the result of a workflow used to answer user queries.\n",
    "    You will be given the user query, the correct answer, the the answer provided by the workflow.\n",
    "    \n",
    "    At the end of your succinct explanation, you should output only a single json object in the following format:\n",
    "    {\"grade\":<GRADE>}\n",
    "    where <GRADE> is an integer between 0 and 10, where 0 is completely irrelevant and/or completely unintelligible, and 10 is perfect in both content and understandability. In between is a continuum of integer grades describing differing levels of imperfection. \n",
    "    \n",
    "    You are EVALUATOR.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_user_prompt = f\"\"\"\n",
    "    Here is the user query:\n",
    "    {question}\n",
    "    \n",
    "    Here is the correct answer:\n",
    "    {correct_answer}\n",
    "    \n",
    "    Here is the answer generated by the workflow:\n",
    "    {given_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    client_response = await llm_client.beta.chat.completions.parse(\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":evaluate_system_prompt},\n",
    "            {\"role\":\"user\"  ,\"content\":evaluate_user_prompt}\n",
    "        ],\n",
    "        model='gpt-4o-mini',\n",
    "        response_format=Evaluation,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    \n",
    "    evaluation_obj = client_response.choices[0].message.parsed\n",
    "    return evaluation_obj.grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b41c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE: Implement execute_workflow_over_data\n",
    "# TODO: Make this asynchronous\n",
    "# TODO: Implement categorical loss\n",
    "async def get_workflow_data_score(workflow,evaluator,datum,llm_client):\n",
    "    q,a = datum\n",
    "    a_hat = (await workflow.start({'data':[q],'llm_client':llm_client}))['data']\n",
    "    score = await evaluator(q,a,a_hat)\n",
    "    return score\n",
    "\n",
    "async def execute_workflow_over_data(workflow,evaluator,data,llm_client):\n",
    "    #scores = []\n",
    "    #costs = []\n",
    "    data_tasks = []\n",
    "    for datum in data:\n",
    "        data_tasks.append(get_workflow_data_score(workflow,evaluator,datum,llm_client))\n",
    "         #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "    scores = await asyncio.gather(*data_tasks)\n",
    "        #costs.append(cost)\n",
    "    return scores#,sum(costs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8319b29-7c67-4149-a622-b4663c66ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_round_scores(W_round,G,Dv,llm_client):\n",
    "    scores = await execute_workflow_over_data(W_round,G,Dv,llm_client) #DONE: Implement execute_workflow_over_data\n",
    "    round_score = sum(scores)/len(scores) #this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\n",
    "    return round_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68ebce8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def get_optimal_workflow(initial_workflow,evaluator,dataset,operators,llm_client,number_of_rounds=20,k=3,early_stopping_rounds=5,scoring_rounds=5,reward_delay=3):\n",
    "    #DONE: Implement evaluator\n",
    "    #Initialize variables\n",
    "    W_0, G, D, N, O, k, n, I = initial_workflow,evaluator,dataset,number_of_rounds,operators,k, \\\n",
    "                                early_stopping_rounds,scoring_rounds\n",
    "    results = []\n",
    "    experiences = Experiences()\n",
    "    topk_W = []\n",
    "    best_W = None\n",
    "    topk_scores = []\n",
    "    best_score = 0\n",
    "    average_score = 0\n",
    "    topk_W_unchanged = 0\n",
    "\n",
    "    keep_alives = {}#{i:None for i in range(reward_delay)}\n",
    "    \n",
    "    #Split dataset\n",
    "    if len(dataset)<50:\n",
    "        Dv,Dt = dataset,dataset\n",
    "    else:\n",
    "        Dv,Dt = split_data(dataset) #DONE: Implement split_data\n",
    "    \n",
    "    #Gather initial scores to determine final validation dataset\n",
    "    score_set_tasks = [execute_workflow_over_data(W_0,G,Dv,llm_client) for _ in range(k)] #DONE: Implement execute_workflow_over_data\n",
    "    score_sets = asyncio.gather(*score_set_tasks)\n",
    "    if len(Dv)>=50:\n",
    "        Dv,variance_threshold = select_high_variance_instances(Dv,score_sets)   #DONE: Implement select_high_variance_instances\n",
    "                                                                        #DONE: Implement determine_variance_threshold\n",
    "    \n",
    "        print(f'{len(Dv)} validation samples selected based on variance threshold {variance_threshold:.2f}')\n",
    "    else:\n",
    "        print(f'All {len(Dv)} validation samples selected for training')\n",
    "    #MAIN LOOP----------------------------------------\n",
    "    #Iterate to improve bestScore\n",
    "    for round_ in (pbar := tqdm(range(N))):\n",
    "        if round_ == 0:\n",
    "            parent = W_0\n",
    "        else:\n",
    "            pbar.set_description(f'Best score:{best_score:.2f} Last round score:{average_score:.2f}')\n",
    "            if len(topk_W) == 1:\n",
    "                parent = topk_W[0]\n",
    "            else:\n",
    "                parent = select_parent(topk_scores,topk_W) #DONE: Implement select_parent\n",
    "        \n",
    "        #Load context for parent and perform optimization forward pass\n",
    "        if round_>0:\n",
    "            parent_maturity = [k for k,v in keep_alives.items() if parent==v[0]]\n",
    "            if parent_maturity:\n",
    "                parent_maturity = parent_maturity[0]\n",
    "            else:\n",
    "                parent_maturity = reward_delay + 1\n",
    "            context = load_context(parent,experiences.experiences) #DONE: Implement load_context\n",
    "            W_round, modification = await optimize(round_,parent,context,O,llm_client,reward_delay,parent_maturity) #DONE: Implement optimize\n",
    "        else:\n",
    "            W_round,modification = parent,''\n",
    "        #Generate validation scores for modified workflow to demonstrate relative performance\n",
    "        round_score_tasks=[]\n",
    "        for i in range(I):\n",
    "            round_score_tasks.append(get_round_scores(W_round,G,Dv,llm_client))\n",
    "        round_scores = await asyncio.gather(*round_score_tasks)\n",
    "        for round_score in round_scores:\n",
    "            results.append((round_,round_score))\n",
    "        \n",
    "        #Capture a new experience for use in future optimization passes by using the modified workflow validation scores as gain\n",
    "        round_scores = [r[1] for r in results if r[0]==round_]\n",
    "        average_score = sum(round_scores)/len(round_scores)\n",
    "        #print(average_score)\n",
    "        experience = experiences.create_experience(W_round,round_,average_score,modification) #DONE: Implement create_experience\n",
    "#         experiences.append(experience) #DONE: Implement Experience(Experiences?) obj in a way that makes retrieval by round easier\n",
    "        \n",
    "        #Save the previous topk_W for the later early-stopping check\n",
    "        prev_topk_W = topk_W\n",
    "        \n",
    "        #Update best W, if applicable this round\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_W = W_round\n",
    "\n",
    "        if reward_delay in keep_alives:\n",
    "            del keep_alives[reward_delay]\n",
    "        keep_alives = {k+1:v for k,v in keep_alives.items()}\n",
    "        keep_alives[0]=(W_round,average_score)\n",
    "\n",
    "        #print(average_score,W_round.extract_chain_info())\n",
    "        \n",
    "        topk_W.append(W_round)\n",
    "        topk_scores.append(average_score)\n",
    "        \n",
    "        #Try to include this round's W_round into topk_W if the W_round's score is within the top k scores\n",
    "        combined = sorted(zip(topk_scores,topk_W),reverse=True)\n",
    "        topk_scores[:],topk_W[:] = zip(*combined)\n",
    "        \n",
    "        #Boot any workflows outside of the top k scores\n",
    "        #if len(topk_scores)>k:\n",
    "        topk_scores = list(topk_scores[:k])\n",
    "        topk_W = list(topk_W[:k])\n",
    "        if round_!=N-1:\n",
    "            topk_W += [keep_alives[k][0] for k in sorted(keep_alives)]\n",
    "            topk_scores += [keep_alives[k][1] for k in sorted(keep_alives)]\n",
    "\n",
    "        #make sure workflows delaying reward \n",
    "        \n",
    "            \n",
    "        #Check if topk_W is the same as the prev topk_W and increment the early stopping counter if so. If not, set counter to 0.\n",
    "        if set([W.id for W in topk_W])==set([prevW.id for prevW in prev_topk_W]):\n",
    "            topk_W_unchanged+=1 #DONE: make sure workflows are being compared based on their IDs \n",
    "                                                    #(since there may not be consistent parity between naive objects)\n",
    "        else:\n",
    "            topk_W_unchanged = 0\n",
    "        \n",
    "        #Stop the workflow improvement early if no new workflows have joined the top k workflows in n rounds\n",
    "        if topk_W_unchanged >= n:\n",
    "            break\n",
    "    #END MAIN LOOP------------------------------------\n",
    "    #return the best workflow and the k-1 next best workflows/the best workflow's score and the k-1 next best workflows' scores\n",
    "    return (best_W,topk_W),(best_score,topk_scores),results,experiences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3ab4b4d-c679-4fa0-9d8c-299e55e784a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install openai --quiet\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e1acc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_workflow = Workflow()\n",
    "operators = {'GenerateText':GenerateText} #registered nodes to be used in optimization\n",
    "llm_client = AsyncOpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db9d42db-cd88-4de1-b2ac-70cab8f94d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    (\"Summarize the key themes discussed in a dataset of customer reviews about a new smartphone.\", \"The reviews primarily focus on the phone's excellent camera quality, long battery life, and sleek design. However, there are several complaints about the high price and lack of a headphone jack.\"),\n",
    "    \n",
    "    (\"Perform sentiment analysis on the following dataset of tweets related to a recent product launch: 'This product is amazing, totally worth the money!', 'Disappointed with the quality. Expected more for the price.', 'Solid performance, but a bit overpriced.'\", \"The sentiments expressed in the dataset are mixed: one tweet is positive, one is negative, and the third expresses neutral sentiment about the product's price.\"),\n",
    "    \n",
    "    (\"Translate the following French sentence into English: 'Le ciel est bleu.'\", \"The translation of 'Le ciel est bleu' is 'The sky is blue.'\"),\n",
    "    \n",
    "    (\"In a dataset of email conversations, identify if the following text is spam: 'Congratulations! You’ve won a free trip to the Bahamas. Click here to claim your prize.'\", \"The text is classified as spam based on the pattern of congratulatory messages and a suspicious link.\"),\n",
    "    \n",
    "    (\"Extract all names of people from the following sentence: 'John and Mary went to Paris for a vacation.'\", \"The extracted names are: John, Mary.\"),\n",
    "    \n",
    "    (\"Given a dataset of news articles, answer the following question: 'Who is the current president of the United States?' from the text: 'In 2024, Joe Biden is serving his second term as president.'\", \"The current president of the United States is Joe Biden.\"),\n",
    "    \n",
    "    # New examples\n",
    "    (\"Identify the sentiment of the following review: 'The hotel was beautiful but the service was terrible.'\", \"The sentiment is mixed, with positive feedback on the hotel itself but negative feedback regarding the service.\"),\n",
    "    \n",
    "    (\"Given a dataset of product descriptions, extract the key features from this description: 'This laptop features a 15.6-inch display, Intel i7 processor, 16GB RAM, and a 512GB SSD.'\", \"The key features are: 15.6-inch display, Intel i7 processor, 16GB RAM, 512GB SSD.\"),\n",
    "    \n",
    "    (\"Translate the following Spanish sentence into English: 'Me gusta mucho la comida mexicana.'\", \"The translation of 'Me gusta mucho la comida mexicana' is 'I really like Mexican food.'\"),\n",
    "    \n",
    "    (\"Classify the following text as either a question or a statement: 'What time does the movie start?'\", \"The text is classified as a question.\"),\n",
    "    \n",
    "    (\"Based on the dataset of historical weather reports, answer the following: 'What was the highest temperature recorded in July 2020?'\", \"The highest temperature recorded in July 2020 was 104°F.\"),\n",
    "    \n",
    "    (\"In a dataset of product reviews, identify if the following review is fake or genuine: 'This product is absolutely amazing and changed my life completely! I recommend it to everyone!'\", \"The review is likely fake due to its exaggerated and overly positive language.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c19f33e-8a41-495c-95c3-a7d1e4b1af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_dataset = [\n",
    "    (\"What is the sum of 7 and 5?\", \"The sum of 7 and 5 is 12.\"),\n",
    "    \n",
    "    (\"What is the product of 8 and 6?\", \"The product of 8 and 6 is 48.\"),\n",
    "    \n",
    "    (\"If you subtract 9 from 15, what do you get?\", \"15 minus 9 equals 6.\"),\n",
    "    \n",
    "    (\"What is the square root of 81?\", \"The square root of 81 is 9.\"),\n",
    "    \n",
    "    (\"What is 25% of 200?\", \"25% of 200 is 50.\"),\n",
    "    \n",
    "    (\"Solve for x: 5x = 20\", \"x equals 4.\"),\n",
    "    \n",
    "    (\"If a rectangle has a length of 10 units and a width of 4 units, what is its area?\", \"The area of the rectangle is 40 square units.\"),\n",
    "    \n",
    "    (\"What is the value of 2 raised to the power of 5?\", \"2 to the power of 5 is 32.\"),\n",
    "    \n",
    "    (\"How many degrees are in a right angle?\", \"A right angle has 90 degrees.\"),\n",
    "    \n",
    "    (\"If 12 is divided by 3, what is the quotient?\", \"The quotient is 4.\"),\n",
    "    \n",
    "    (\"A car travels 60 miles in 2 hours. What is its average speed in miles per hour?\", \"The average speed is 30 miles per hour.\"),\n",
    "    \n",
    "    (\"What is the perimeter of a square with a side length of 5 units?\", \"The perimeter is 20 units.\"),\n",
    "    \n",
    "    (\"How many centimeters are there in 1 meter?\", \"There are 100 centimeters in 1 meter.\"),\n",
    "    \n",
    "    (\"What is the greatest common divisor (GCD) of 24 and 36?\", \"The GCD of 24 and 36 is 12.\"),\n",
    "    \n",
    "    (\"Convert 0.75 into a fraction in its simplest form.\", \"0.75 as a fraction in simplest form is 3/4.\"),\n",
    "    \n",
    "    (\"If a triangle has angles of 60° and 70°, what is the measure of the third angle?\", \"The third angle measures 50°.\"),\n",
    "    \n",
    "    (\"How many faces does a cube have?\", \"A cube has 6 faces.\"),\n",
    "    \n",
    "    (\"Solve for y: 3y + 7 = 22\", \"y equals 5.\"),\n",
    "    \n",
    "    (\"What is the average of the numbers 4, 8, and 12?\", \"The average is 8.\"),\n",
    "    \n",
    "    (\"If you flip a fair coin, what is the probability of getting heads?\", \"The probability is 1/2.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd8fd917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 20 validation samples selected for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aebd947fe15410faecf2a718355571d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (W_star,topk_W),(best_score,topk_scores),results,experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_optimal_workflow(initial_workflow,evaluate,math_dataset,operators,llm_client,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,number_of_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 60\u001b[0m, in \u001b[0;36mget_optimal_workflow\u001b[0;34m(initial_workflow, evaluator, dataset, operators, llm_client, number_of_rounds, k, early_stopping_rounds, scoring_rounds, reward_delay)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(I):\n\u001b[1;32m     59\u001b[0m     round_score_tasks\u001b[38;5;241m.\u001b[39mappend(get_round_scores(W_round,G,Dv,llm_client))\n\u001b[0;32m---> 60\u001b[0m round_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mround_score_tasks)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m round_score \u001b[38;5;129;01min\u001b[39;00m round_scores:\n\u001b[1;32m     62\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((round_,round_score))\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/asyncio/tasks.py:375\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/asyncio/tasks.py:306\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    304\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m, in \u001b[0;36mget_round_scores\u001b[0;34m(W_round, G, Dv, llm_client)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_round_scores\u001b[39m(W_round,G,Dv,llm_client):\n\u001b[0;32m----> 2\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m execute_workflow_over_data(W_round,G,Dv,llm_client) \u001b[38;5;66;03m#DONE: Implement execute_workflow_over_data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     round_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(scores)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(scores) \u001b[38;5;66;03m#this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m round_score\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mexecute_workflow_over_data\u001b[0;34m(workflow, evaluator, data, llm_client)\u001b[0m\n\u001b[1;32m     15\u001b[0m     data_tasks\u001b[38;5;241m.\u001b[39mappend(get_workflow_data_score(workflow,evaluator,datum,llm_client))\n\u001b[1;32m     16\u001b[0m      \u001b[38;5;66;03m#this works for a continuous loss/gain (e.g. answer quality/lack of), TODO: Implement categorical loss\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mdata_tasks)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#costs.append(cost)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/asyncio/tasks.py:375\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mget_workflow_data_score\u001b[0;34m(workflow, evaluator, datum, llm_client)\u001b[0m\n\u001b[1;32m      5\u001b[0m q,a \u001b[38;5;241m=\u001b[39m datum\n\u001b[1;32m      6\u001b[0m a_hat \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m workflow\u001b[38;5;241m.\u001b[39mstart({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m:[q],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_client\u001b[39m\u001b[38;5;124m'\u001b[39m:llm_client}))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluator(q,a,a_hat)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(question, correct_answer, given_answer)\u001b[0m\n\u001b[1;32m      7\u001b[0m evaluate_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mYou are being given a chance to evaluate the result of a workflow used to answer user queries.\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mYou will be given the user query, the correct answer, the the answer provided by the workflow.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mYou are EVALUATOR.\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     18\u001b[0m evaluate_user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mHere is the user query:\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgiven_answer\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m client_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m     30\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:evaluate_system_prompt},\n\u001b[1;32m     32\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m  ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:evaluate_user_prompt}\n\u001b[1;32m     33\u001b[0m     ],\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mEvaluation,\n\u001b[1;32m     36\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m evaluation_obj \u001b[38;5;241m=\u001b[39m client_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mparsed\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_obj\u001b[38;5;241m.\u001b[39mgrade\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/resources/beta/chat/completions.py:421\u001b[0m, in \u001b[0;36mAsyncCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    416\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    417\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    418\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    423\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    424\u001b[0m         {\n\u001b[1;32m    425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    426\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    427\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    428\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    429\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    430\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    431\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    432\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    433\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    436\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    437\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    440\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: _type_to_response_format(response_format),\n\u001b[1;32m    441\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    450\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    451\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    452\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    453\u001b[0m         },\n\u001b[1;32m    454\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    455\u001b[0m     ),\n\u001b[1;32m    456\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    457\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    458\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    459\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    460\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    461\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    462\u001b[0m     ),\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001b[1;32m    466\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1838\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1826\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1834\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1835\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1836\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[0;32m-> 1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1532\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1533\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1534\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1535\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1536\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1537\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1538\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1618\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1619\u001b[0m         input_options,\n\u001b[1;32m   1620\u001b[0m         cast_to,\n\u001b[1;32m   1621\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1622\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1623\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1624\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1625\u001b[0m     )\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1665\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1661\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1666\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1667\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1668\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1669\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1670\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1671\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1618\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1619\u001b[0m         input_options,\n\u001b[1;32m   1620\u001b[0m         cast_to,\n\u001b[1;32m   1621\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1622\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1623\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1624\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1625\u001b[0m     )\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1665\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1661\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1666\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1667\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1668\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1669\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1670\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1671\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python313/lib/python3.13/site-packages/openai/_base_client.py:1633\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1632\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1636\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1637\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1642\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "(W_star,topk_W),(best_score,topk_scores),results,experiences = await get_optimal_workflow(initial_workflow,evaluate,math_dataset,operators,llm_client,k=5,number_of_rounds=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43758e-7035-4696-9698-a7957631168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da955c34-520f-4da4-8997-5c79b6a7f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nodes = []\n",
    "best_w = topk_W[-1]\n",
    "while best_w.next:\n",
    "    best_nodes.append(best_w)\n",
    "    best_w = best_w.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4df32c-1420-43b8-a830-e4da4d2f15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652abd4a-505b-494f-9899-ce054b6500fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(node.__dict__) for node in best_nodes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c9014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# workflow_test = Workflow()\n",
    "# workflow_test.add_node(GenerateText,'The GenerateText node is designed to create textual responses based on the input it receives. It can synthesize information, provide explanations, or generate creative content depending on the context of the user query. This node will enhance the workflow by ensuring that the responses are not only relevant but also articulated in a coherent and engaging manner.', \n",
    "#                        \"Based on the user query, generate a detailed and informative text response that addresses the user's needs and provides additional context or examples where necessary.\",\n",
    "#                       'Add a new GenerateText to the workflow')\n",
    "# print(workflow_test.extract_chain_info())\n",
    "# execute_workflow_over_data(workflow_test,evaluate,dataset,llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f86204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c85668e",
   "metadata": {},
   "source": [
    "### Future Enhancements\n",
    "We can optimize one layer/level of operators.\n",
    "\n",
    "What about two layers of operators? (e.g. operators nested in operators)\n",
    "\n",
    "What about N layers of operators?\n",
    "\n",
    "\n",
    "e.g. can we implement hierarchical optimization so that each and every step in the chain is tightly fit to purpose\n",
    "\n",
    "e.g. can we construct operators from smaller units on the fly?\n",
    "\n",
    "e.g. are there general operators such that these operators can be constructed of these operators at any level of depth\n",
    "\n",
    "\n",
    "We have implemented breadth (operator columns) - can we implement depth (operator rows)?\n",
    "\n",
    "Can we do it just as fast as breadth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
